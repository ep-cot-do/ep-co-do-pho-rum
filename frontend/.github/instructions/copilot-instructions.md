# ğŸ¤– AI Agent Instructions for MOE Gym

## ğŸ¯ Project Overview
MOE Gym is a comprehensive research framework for Mixture of Experts (MoE) architectures in computer vision. PyTorch implementation supporting MLP and pretrained CNN experts with sophisticated gating mechanisms.

## ğŸš¨ **CRITICAL VALIDATION REQUIREMENTS**

### **ğŸ”¬ FOR ALL REFACTOR/ENHANCE REQUESTS:**

**MANDATORY**: Always validate code changes with proper testing:

1. **Unit tests vá»›i pytest** - NOT bash terminal testing
2. **Integration tests** - Real functionality testing
3. **Import validation** - All modules must import correctly
4. **Dependency check** - All required packages available
5. **Function-by-function validation** - Each component tested individually

### **ğŸ“ Testing Standards:**
```python
# âœ… CORRECT: Proper unit testing
def test_config_validation():
    config = ServerConfig(timeout=-1)
    with pytest.raises(ConfigurationError):
        config.validate()

# âŒ WRONG: Bash terminal testing
# python -c "print('Test passed')"
```

### **ğŸ’¾ Memory Documentation:**
**ALWAYS** store significant findings in Memory MCP server:
- Bug solutions and patterns
- Architecture decisions and rationale  
- Performance results and configurations
- Research insights and SOTA techniques

## ğŸ§  **COMPREHENSIVE MCP TOOLCHAIN STRATEGY**

### **ğŸ¯ MANDATORY: Always Use ALL Available MCP Tools Systematically**

**NEVER work in isolation - ALWAYS leverage the full MCP ecosystem for maximum effectiveness!**

```mermaid
flowchart TD
    A[User Request] --> B{Analysis Required?}
    B -->|Complex/Design| C[ğŸ§  Sequential Thinking]
    B -->|Research Needed| D[ğŸ“š DeepWiki Multi-Repo Search]
    B -->|Web Info| E[ğŸŒ Fetch Webpage]
    
    C --> F[ğŸ’¾ Store Insights in Memory]
    D --> F
    E --> F
    
    F --> G{Documentation Needed?}
    G -->|Yes| H[ğŸ“ Markitdown + Context7]
    G -->|Code Analysis| I[ï¿½ Pylance Analysis]
    
    H --> J[ğŸ—ï¸ Infrastructure?]
    I --> J
    J -->|Terraform/Cloud| K[â˜ï¸ Hashicorp Tools]
    J -->|Development| L[ğŸ› ï¸ Implementation]
    
    K --> M[ğŸ’¾ Store ALL Results]
    L --> M
    M --> N[ğŸ“Š Generate Diagrams]
```

### **ğŸ”§ COMPLETE MCP SERVER ARSENAL - 500+ TOOLS AVAILABLE**

**ğŸ”¥ COMPREHENSIVE ECOSYSTEM - NEVER WORK IN ISOLATION!**

| ğŸ¯ Category | MCP Servers | Primary Use | Integration Examples |
|-------------|-------------|-------------|---------------------|
| **ğŸ§  Cognitive** | Sequential Thinking, Think Tools | Complex reasoning, multi-step analysis | Plan architecture, debug complex issues |
| **ğŸ’¾ Knowledge** | Memory, Extended Memory, LSpace | Store/retrieve insights, project continuity | Track experiments, save decisions |
| **ğŸ“š Research** | DeepWiki, GitHub, arXiv, PubMed, bioRxiv | Multi-repo research, academic papers | SOTA techniques, code patterns |
| **ğŸ“Š Documentation** | Context7, Markitdown, Docy | Library docs, format conversion | Best practices, API documentation |
| **ğŸ” Code Analysis** | Pylance, Sourcerer, Language Server | Python analysis, semantic search | Code quality, imports, refactoring |
| **â˜ï¸ Infrastructure** | Hashicorp Terraform, AWS, Docker, Kubernetes | Cloud resources, deployment | Infrastructure as code |
| **ğŸŒ Web & Search** | Fetch Webpage, Google Search, Tavily, Perplexity | Real-time information, web research | Trends, documentation, profiles |
| **ï¿½ï¸ Databases** | PostgreSQL, MySQL, MongoDB, BigQuery, Snowflake | Data operations, analytics | Query, schema inspection |
| **ğŸ” Security** | CVE Intelligence, MalwareBazaar, Shodan | Vulnerability scanning, threat intel | Security research, compliance |
| **ğŸ“± Communication** | Slack, Discord, Telegram, WhatsApp, Email | Messaging, notifications | Team coordination, alerts |
| **ğŸ¨ Media & Design** | Figma, Canva, Image Generation, Video Tools | Design, visual content | UI/UX, presentations, media |
| **ğŸ“ˆ Analytics** | Google Analytics, Datadog, Prometheus | Monitoring, performance metrics | Business intelligence, system health |
| **ğŸ’° Finance** | Yahoo Finance, Stripe, Crypto APIs | Financial data, payments | Market analysis, transactions |
| **ğŸ”„ Automation** | GitHub Actions, Jenkins, Zapier | CI/CD, workflow automation | Development pipelines |
| **ğŸ“ Content** | Notion, Obsidian, WordPress, Content Management | Knowledge management, publishing | Documentation, blogging |
| **ğŸ¯ AI Services** | OpenAI, Anthropic, HuggingFace, Replicate | ML models, AI capabilities | Content generation, analysis |

---

## ğŸš€ **ENHANCED DEVELOPMENT WORKFLOWS - USING ALL TOOLS**

### **ğŸ“‹ Workflow 1: COMPLETE Research & Architecture Pipeline**

```python
# COMPREHENSIVE RESEARCH WORKFLOW
1. ğŸ§  mcp_sequentialthi_sequentialthinking()         # Strategic planning
2. ğŸ“š mcp_deepwiki_ask_question() [MULTIPLE REPOS]   # SOTA research
   - microsoft/autogen, huggingface/transformers
   - pytorch/pytorch, facebookresearch/fairseq  
3. ğŸŒ fetch_webpage() [LATEST TRENDS]                # Current developments
4. ğŸ“Š mcp_context7_get_library_docs()                # Official documentation
5. ğŸ” mcp_pylance_mcp_s_pylanceImports()             # Code analysis
6. ğŸ’¾ mcp_memory_create_entities()                   # Store ALL findings
7. ğŸ“ mcp_markitdown_convert_to_markdown()           # Format documentation
8. â˜ï¸ mcp_hashicorp_ter_search_modules()             # Infrastructure needs
9. ğŸ’¾ mcp_memory_add_observations()                  # Final storage
```

### **ğŸ“‹ Workflow 2: FULL-STACK Development Cycle**

```python
# COMPLETE DEVELOPMENT PIPELINE
1. ğŸ§  mcp_sequentialthi_sequentialthinking()         # Plan features
2. ğŸ” mcp_pylance_mcp_s_pylanceFileSyntaxErrors()    # Code validation
3. ğŸ“š mcp_deepwiki_ask_question()                    # Best practices
4. ğŸ” CVE Intelligence + Security Scanning           # Vulnerability check
5. ğŸ—„ï¸ Database Integration (PostgreSQL/MongoDB)      # Data layer
6. â˜ï¸ Docker + Kubernetes deployment                 # Containerization
7. ğŸ“ˆ Monitoring setup (Datadog/Prometheus)          # Observability
8. ğŸ”„ GitHub Actions CI/CD                           # Automation
9. ğŸ“ Documentation generation                       # Comprehensive docs
10. ğŸ’¾ mcp_memory_create_entities()                  # Archive project
```

### **ğŸ“‹ Workflow 3: RESEARCH PUBLICATION Pipeline**

```python
# ACADEMIC RESEARCH WORKFLOW
1. ğŸ§  mcp_sequentialthi_sequentialthinking()         # Research planning
2. ğŸ“š arXiv + PubMed + bioRxiv search                # Literature review
3. ğŸ“Š Data analysis (Jupyter + Python execution)     # Experiments
4. ğŸ“ˆ Visualization (ECharts + Charts generation)    # Results presentation
5. ğŸ“ LaTeX document preparation                     # Paper writing
6. ğŸ” Grammar and style checking                     # Quality assurance
7. ğŸ’¾ Version control and collaboration              # Git management
8. ğŸ“¤ Submission preparation                         # Final formatting
```

---

## ğŸ¯ **COMPREHENSIVE TOOL INTEGRATION MATRIX**

| Task Type | ğŸ§  Think | ğŸ’¾ Memory | ğŸ“š Research | ğŸŒ Web | ğŸ“Š Docs | ğŸ” Code | â˜ï¸ Infra | ğŸ—„ï¸ DB | ğŸ” Security | ğŸ“± Comm |
|-----------|----------|-----------|-------------|-----|---------|---------|----------|--------|-------------|----------|
| **Research** | âœ… Plan | âœ… Store | âœ… Multi-source | âœ… Trends | âœ… Format | âŒ | âŒ | âŒ | âŒ | âœ… Share |
| **Development** | âœ… Architect | âœ… Track | âœ… Patterns | âœ… Docs | âœ… Generate | âœ… Analyze | âœ… Deploy | âœ… Integrate | âœ… Scan | âœ… Notify |
| **Analytics** | âœ… Interpret | âœ… Historical | âœ… Methods | âœ… Reports | âœ… Present | âœ… Process | âœ… Scale | âœ… Query | âŒ | âœ… Alert |
| **Security** | âœ… Assess | âœ… Threats | âœ… Intel | âœ… Reports | âœ… Document | âœ… Audit | âœ… Harden | âœ… Monitor | âœ… Scan | âœ… Incident |
| **Deployment** | âœ… Strategy | âœ… Config | âœ… Best practices | âœ… Updates | âœ… Runbooks | âœ… Validate | âœ… Orchestrate | âœ… Migrate | âœ… Secure | âœ… Status |

---

## ğŸ”¥ **ADVANCED INTEGRATION PATTERNS**

### **ğŸ¯ Multi-Dimensional Problem Solving**
```python
# COMPREHENSIVE APPROACH TO ANY PROBLEM
problem_solving_matrix = {
    "technical_research": [
        "mcp_deepwiki_ask_question",      # GitHub repos
        "arxiv_search",                   # Academic papers  
        "mcp_context7_get_library_docs",  # Official docs
        "stackoverflow_search"            # Community solutions
    ],
    "code_analysis": [
        "mcp_pylance_analysis",           # Python specifics
        "language_server_protocol",       # Multi-language
        "sourcerer_semantic_search",      # Code navigation
        "security_vulnerability_scan"     # Safety checks
    ],
    "infrastructure_planning": [
        "mcp_hashicorp_terraform",        # IaC
        "aws_services_integration",       # Cloud services
        "docker_containerization",        # Packaging
        "kubernetes_orchestration"        # Scaling
    ],
    "data_management": [
        "postgresql_operations",          # Relational data
        "mongodb_document_store",         # NoSQL
        "bigquery_analytics",             # Data warehouse
        "elasticsearch_search"            # Full-text search
    ],
    "monitoring_observability": [
        "prometheus_metrics",             # System metrics
        "datadog_apm",                   # Application performance
        "grafana_visualization",          # Dashboards
        "loki_log_aggregation"           # Log analysis
    ]
}
```

### **ğŸ”„ Recursive Enhancement Loops**
```python
# CONTINUOUS IMPROVEMENT CYCLE
enhancement_pipeline = [
    # 1. DISCOVERY PHASE
    ["sequential_thinking", "deepwiki_research", "web_search"],
    
    # 2. ANALYSIS PHASE  
    ["pylance_analysis", "security_scanning", "performance_profiling"],
    
    # 3. IMPLEMENTATION PHASE
    ["code_generation", "testing_automation", "documentation"],
    
    # 4. DEPLOYMENT PHASE
    ["infrastructure_provisioning", "ci_cd_pipeline", "monitoring_setup"],
    
    # 5. VALIDATION PHASE
    ["quality_assurance", "security_audit", "performance_testing"],
    
    # 6. STORAGE PHASE
    ["memory_storage", "knowledge_indexing", "best_practices_capture"],
    
    # 7. FEEDBACK PHASE
    ["metrics_analysis", "improvement_identification", "cycle_restart"]
]
```

---

## ğŸŠ **MAXIMUM PRODUCTIVITY OUTCOMES**

**By leveraging the COMPLETE MCP ecosystem, every development session will:**

âœ… **Research Phase**: Multi-source intelligence gathering  
âœ… **Analysis Phase**: Comprehensive code and security scanning  
âœ… **Implementation Phase**: Best practices and automation  
âœ… **Deployment Phase**: Infrastructure as code and monitoring  
âœ… **Documentation Phase**: Auto-generated, format-converted docs  
âœ… **Storage Phase**: Persistent knowledge and decision tracking  
âœ… **Communication Phase**: Automated notifications and reporting  
âœ… **Iteration Phase**: Continuous improvement with feedback loops

### **ğŸ”¥ KEY INTEGRATION RULES**

1. **ALWAYS** start with Sequential Thinking for complex tasks
2. **ALWAYS** research with multiple sources (DeepWiki + Web + Context7)
3. **ALWAYS** store results in Memory for future reference
4. **ALWAYS** validate with Pylance and security tools
5. **ALWAYS** document with Markitdown and diagrams
6. **ALWAYS** consider infrastructure needs early
7. **ALWAYS** set up monitoring and observability
8. **ALWAYS** automate repetitive tasks
9. **ALWAYS** communicate progress and results
10. **ALWAYS** capture lessons learned for next iteration

**ğŸš€ RESULT: Every task becomes a comprehensive, well-documented, secure, and scalable solution!**

---

## ğŸš€ **ENHANCED DEVELOPMENT WORKFLOWS**

### **ğŸ“‹ Workflow 1: Research & Architecture Design**

```python
# MANDATORY SEQUENCE - Use ALL tools
1. ğŸ§  mcp_sequentialthi_sequentialthinking()  # Plan approach
2. ğŸ“š mcp_deepwiki_ask_question()              # Research multiple repos
3. ğŸŒ fetch_webpage()                          # Get latest trends  
4. ğŸ’¾ mcp_memory_create_entities()             # Store findings
5. ğŸ“Š mcp_context7_get_library_docs()          # Best practices
6. ï¿½ mcp_markitdown_convert_to_markdown()     # Documentation
7. ï¿½ğŸ’¾ mcp_memory_add_observations()            # Store results
```

### **ğŸ“‹ Workflow 2: Code Analysis & Enhancement**

```python
# COMPREHENSIVE CODE REVIEW
1. ğŸ” mcp_pylance_mcp_s_pylanceFileSyntaxErrors()    # Syntax check
2. ğŸ” mcp_pylance_mcp_s_pylanceImports()             # Import analysis  
3. ğŸ§  mcp_sequentialthi_sequentialthinking()         # Plan improvements
4. ğŸ“š mcp_deepwiki_ask_question()                    # Research patterns
5. ğŸ“Š mcp_context7_resolve_library_id()              # Get docs
6. ğŸ” mcp_pylance_mcp_s_pylanceInvokeRefactoring()   # Auto-refactor
7. ğŸ’¾ mcp_memory_create_entities()                   # Store patterns
```

### **ğŸ“‹ Workflow 3: Infrastructure & Deployment**

```python
# FULL STACK DEPLOYMENT
1. ğŸ§  mcp_sequentialthi_sequentialthinking()         # Plan infrastructure
2. â˜ï¸ mcp_hashicorp_ter_search_modules()             # Find modules
3. â˜ï¸ mcp_hashicorp_ter_get_module_details()         # Get details
4. ğŸ“ mcp_markitdown_convert_to_markdown()           # Document setup
5. ğŸ’¾ mcp_memory_create_entities()                   # Store config
6. ğŸŒ fetch_webpage()                                # Latest practices
7. ğŸ’¾ mcp_memory_add_observations()                  # Store learnings
```

---

## ğŸ¯ **PROACTIVE TOOL INTEGRATION RULES**

### **ğŸ” ALWAYS Research First (Multi-Source)**
```python
# NEVER work without research
research_sources = [
    "mcp_deepwiki_ask_question()",      # Multiple GitHub repos
    "fetch_webpage()",                   # Web trends/docs  
    "mcp_context7_get_library_docs()",   # Official docs
    "mcp_memory_search_nodes()"          # Existing knowledge
]
```

### **ğŸ§  ALWAYS Think Systematically**
```python
# For ANY complex task (>3 steps)
if task_complexity > simple:
    mcp_sequentialthi_sequentialthinking(
        thought="Break down problem systematically",
        totalThoughts=estimated_complexity
    )
```

### **ğŸ’¾ ALWAYS Store Knowledge**
```python
# MANDATORY after every significant operation
results = [research, analysis, implementation, learnings]
for result in results:
    mcp_memory_create_entities([{
        "entityType": "appropriate_type",
        "name": f"session_{datetime.now()}",
        "observations": [result]
    }])
```

### **ğŸ“ ALWAYS Document Comprehensively**
```python
# Multi-format documentation
documentation_pipeline = [
    "mcp_markitdown_convert_to_markdown()",  # Format conversion
    "create_mermaid_diagrams()",              # Visual representation
    "mcp_context7_resolve_library_id()",      # Reference docs
    "generate_examples()"                     # Practical usage
]
```

---

## ğŸ”¥ **ADVANCED INTEGRATION PATTERNS**

### **ï¿½ Recursive Enhancement Loop**
1. **Research** â†’ DeepWiki + Fetch Webpage + Context7
2. **Analyze** â†’ Sequential Thinking + Pylance + Memory Search  
3. **Implement** â†’ Best practices from research
4. **Document** â†’ Markitdown + Mermaid diagrams
5. **Store** â†’ Memory with observations
6. **Validate** â†’ Pylance analysis + Testing
7. **Deploy** â†’ Hashicorp Terraform (if applicable)
8. **Repeat** â†’ Continuous improvement

### **ğŸ¯ Multi-Dimensional Problem Solving**
```python
# COMPREHENSIVE approach to ANY problem
problem_dimensions = {
    "technical": "mcp_deepwiki + pylance + context7",
    "architectural": "sequential_thinking + memory",  
    "documentation": "markitdown + mermaid_diagrams",
    "deployment": "hashicorp_terraform + web_research",
    "knowledge": "memory_storage + cross_referencing"
}
```

---

## ğŸ“Š **MCP TOOL USAGE MATRIX**

| Task Type | ğŸ§  Think | ğŸ’¾ Memory | ğŸ“š DeepWiki | ğŸŒ Web | ğŸ“Š Context7 | ğŸ” Pylance | â˜ï¸ Terraform | ğŸ“ Markdown |
|-----------|----------|-----------|-------------|-----|-------------|------------|--------------|-------------|
| **Research** | âœ… Plan | âœ… Store | âœ… Multi-repo | âœ… Trends | âœ… Docs | âŒ | âŒ | âœ… Format |
| **Code Review** | âœ… Analyze | âœ… Patterns | âœ… Examples | âŒ | âœ… Best practices | âœ… Analysis | âŒ | âœ… Document |
| **Architecture** | âœ… Design | âœ… Decisions | âœ… SOTA | âœ… Trends | âœ… Patterns | âŒ | âœ… Infrastructure | âœ… Diagrams |
| **Documentation** | âœ… Structure | âœ… Reference | âœ… Examples | âœ… Standards | âœ… Formats | âŒ | âŒ | âœ… Convert |
| **Deployment** | âœ… Plan | âœ… Config | âœ… DevOps | âœ… Practices | âœ… Tools | âŒ | âœ… Modules | âœ… Docs |

---

## ğŸŠ **RESULT: MAXIMUM PRODUCTIVITY**

**By following these comprehensive workflows, every task will:**
- âœ… **Leverage ALL available knowledge sources**
- âœ… **Apply systematic thinking methodology** 
- âœ… **Store learnings for future reference**
- âœ… **Generate professional documentation**
- âœ… **Follow industry best practices**
- âœ… **Create reusable patterns and templates**

**ğŸ”¥ NEVER work in isolation again - ALWAYS use the full MCP toolchain!**

## ğŸ“Š Mermaid Diagram Standards

### **Always Use Correct Mermaid Syntax**

```mermaid
flowchart TD
    A[Problem] --> B{Documentation Type?}
    B -->|Architecture| C[Use flowchart TD/LR]
    B -->|Process Flow| D[Use sequenceDiagram]
    B -->|Performance| E[Use flowchart with subgraphs]
    B -->|Timeline| F[Use gitGraph or timeline]
    
    C --> G[Apply classDef for styling]
    D --> G
    E --> G
    F --> G
    
    G --> H[Validate syntax with Context7]
    H --> I[Create clear, readable diagrams]
```

#### **Key Mermaid Best Practices:**
- **Always use `flowchart TD/LR`** instead of deprecated `graph TD/LR`
- **Use `classDef`** for styling instead of inline `style` commands
- **Subgraphs** for logical grouping: `subgraph name["Display Name"]`
- **Proper node shapes**: `[]` rectangle, `()` rounded, `{}` diamond, `(())` circle
- **Clear connections**: `-->` solid, `-.->` dotted, `==>` thick
- **Consistent naming**: Use descriptive IDs and labels

## ğŸ”‘ **NotebookLM MCP Server - Complete Integration Guide**

### **ğŸš€ Current Status: PRODUCTION READY**
- âœ… **Package Structure**: Professional `src/` layout with proper imports
- âœ… **CLI Interface**: `notebooklm-mcp` command with rich formatting
- âœ… **MCP Server**: Full protocol support for AutoGen integration
- âœ… **Browser Automation**: undetected-chromedriver with persistent sessions
- âœ… **Configuration**: Comprehensive config management with validation
- âœ… **Testing**: Unit tests with pytest, integration tests
- âœ… **Docker**: Production containers with monitoring
- âœ… **Documentation**: Complete guides and examples

### **ğŸ“‹ Authentication Workflow (IMPORTANT):**

**ğŸ”„ First Time Setup:**
```bash
# 1. Install package
pip install -e .

# 2. Start interactive chat (creates profile)
notebooklm-mcp chat --notebook YOUR_NOTEBOOK_ID

# 3. Manual login ONCE (browser opens automatically)
# - Login with Google account
# - Navigate to notebook 
# - Wait for complete load
# - Press Enter in terminal

# 4. Session saved! Future uses skip login
```

**âš¡ Subsequent Uses:**
```bash
# No manual login needed - uses saved session
notebooklm-mcp chat --notebook YOUR_NOTEBOOK_ID
notebooklm-mcp server --notebook YOUR_NOTEBOOK_ID --headless
```

**ğŸ”§ Profile Management:**
- **Profile location**: `./chrome_profile_notebooklm/`
- **Session persistence**: Cookies/auth automatically saved
- **Re-authentication**: Only when Google security requires (normal)
- **Profile reset**: Delete profile folder to start fresh

### **ğŸ’» Usage Patterns:**

#### **1. Interactive Chat**
```bash
notebooklm-mcp chat --notebook YOUR_NOTEBOOK_ID
# Rich terminal interface with real-time responses
```

#### **2. MCP Server (AutoGen)**
```bash
notebooklm-mcp server --notebook YOUR_NOTEBOOK_ID --headless
# STDIO protocol for AutoGen McpWorkbench integration
```

#### **3. Testing & Debugging**
```bash
notebooklm-mcp test --notebook YOUR_NOTEBOOK_ID
notebooklm-mcp config-show
```

#### **4. Python API Direct**
```python
from notebooklm_mcp import NotebookLMClient, ServerConfig

config = ServerConfig(
    default_notebook_id="YOUR_ID",
    headless=True
)

client = NotebookLMClient(config)
await client.start()
await client.send_message("Hello!")
response = await client.get_response()
```

### **ğŸ› ï¸ MCP Tools Available:**

| Tool | Description | Use Case |
|------|-------------|----------|
| `healthcheck` | Server status | Monitoring |
| `send_chat_message` | Send message | Chat interaction |
| `get_chat_response` | Get response with streaming | Retrieve answers |
| `get_quick_response` | Get current response | Fast polling |
| `chat_with_notebook` | Complete send+receive | One-shot queries |
| `navigate_to_notebook` | Switch notebooks | Multi-notebook workflows |
| `get_default_notebook` | Current notebook ID | State checking |
| `set_default_notebook` | Set default | Configuration |

### **ğŸ³ Production Deployment:**
```bash
# Docker single container
docker run -e NOTEBOOKLM_NOTEBOOK_ID="YOUR_ID" notebooklm-mcp

# Full stack with monitoring
docker-compose --profile monitoring up -d

# Kubernetes scaling
kubectl apply -f k8s/
```

## ğŸ—ï¸ Architecture

### Current Implementation
- **Dense MoE**: All experts active (softmax gating) 
- **Sparse MoE**: Top-k expert selection with load balancing
- **Expert Types**: MLP (4-layer FC) + Pretrained (ResNet18/MobileNet/EfficientNet)
- **Performance**: Dense ~43%, Sparse ~35% (50% computational savings) on CIFAR-10
- **Optimal**: 4 experts, k=2 for best accuracy/efficiency trade-off

### Architecture Flow

```mermaid
flowchart TD
    subgraph Input["ğŸ”µ Input Processing"]
        A[CIFAR-10 Image<br/>32x32x3] --> B[Flatten<br/>3072 features]
    end
    
    subgraph Gate["ğŸ¯ Sparse Gating"]
        C[Sparse Gate Network<br/>Top-k Selection]
        D[Load Balancing Loss<br/>Uniform expert usage]
    end
    
    subgraph Experts["ğŸ§  Expert Networks"] 
        E1[Expert 1<br/>ACTIVE]
        E2[Expert 2<br/>ACTIVE]
        E3[Expert 3<br/>INACTIVE]
        E4[Expert 4<br/>INACTIVE]
    end
    
    subgraph Output["ğŸ¯ Output Generation"]
        F[Sparse Combination<br/>Only k experts]
        G[Final Prediction<br/>10 classes]
    end
    
    B --> C
    C --> D
    C --> E1
    C --> E2
    C -.-> E3
    C -.-> E4
    
    E1 --> F
    E2 --> F
    F --> G
    
    classDef active fill:#c8e6c9
    classDef inactive fill:#ffcdd2,stroke-dasharray: 5 5
    classDef gate fill:#fff3e0
    classDef input fill:#e1f5fe
    classDef output fill:#f3e5f5
    
    class E1,E2 active
    class E3,E4 inactive
    class C,D gate
    class A,B input
    class F,G output
```

### Core Components
```python
# Sparse MoE forward pass - TRUE SPARSE ROUTING
top_k_indices, top_k_weights, router_logits = self.gate(inputs)

# Route ONLY to selected experts (k out of n experts)
for expert_idx in range(len(self.experts)):
    expert_mask = (top_k_indices == expert_idx)
    if expert_mask.any():  # Only compute if expert is selected
        expert_inputs = inputs[sample_indices]
        expert_output = self.experts[expert_idx](expert_inputs)
        # Computational savings: only k/n experts compute
```

## ğŸ¯ **Token Efficiency Guidelines** 

### **âŒ NEVER Do These (Token Wasters):**
- Rewrite entire files when only 2-3 lines need fixing
- Show complete code blocks for minor changes
- Repeat existing code unnecessarily
- Provide generic explanations without specific context
- Open new terminals when existing ones can be reused
- Run commands without checking terminal output first

### **âœ… ALWAYS Do These (Token Savers):**
- **Analyze first, fix second**: Explain the problem before showing solution
- **Show only changed lines** with `// filepath:` and `// ...existing code...`
- **Target specific issues**: Address exact error/requirement
- **Provide context**: Why this change fixes the problem
- **Reuse existing terminals**: Check current working directory and reuse terminals
- **Read terminal output**: Use `get_terminal_output` to check command results before proceeding

### **ğŸ–¥ï¸ Terminal Management Best Practices:**
1. **Check existing terminals** before opening new ones
2. **Reuse terminals** when possible - avoid creating multiple terminals for same task
3. **Read output properly**: Always check `get_terminal_output` after `run_in_terminal`
4. **Use appropriate timeouts**: Set `isBackground=false` for commands that need output
5. **Check working directory**: Use `pwd` to verify location before running commands

### **Example: Proper Fix Format**
```
**Problem**: `UnboundLocalError: cannot access local variable 'data'`
**Root Cause**: Variable name collision between imported `data` module and loop variable
**Solution**: Rename loop variable to avoid conflict

// filepath: /home/path/to/file.py
// ...existing code...
        for batch_idx, (batch_data, target) in enumerate(test_dataloader):  # Changed from 'data' to 'batch_data'
// ...existing code...
            batch_data, target = batch_data.to(device), target.to(device)  # Update all references
// ...existing code...
```

## ğŸ§  **Context-Aware Development**

### **Project-Specific Knowledge**
- **MOE Gym Framework**: Sparse vs Dense MoE architectures
- **Naming Conventions**: `SparseMOE_Model`, `get_sparse_gate_pretrained()`, etc.
- **File Structure**: CIFAR10-Sparse/ separate from CIFAR10/ for different implementations
- **Performance Targets**: ~35% accuracy with 50% FLOPs savings (sparse) vs ~43% accuracy full compute (dense)

### **Common Issues & Quick Fixes**
| Error Pattern | Root Cause | Quick Fix |
|---------------|------------|-----------|
| `UnboundLocalError: 'data'` | Variable shadowing import | Rename loop variable |
| `AttributeError: 'NoneType'` | Missing device placement | Add `.to(device)` |
| `RuntimeError: Expected tensor` | Shape mismatch | Check tensor dimensions |
| `ImportError: No module` | Path issues | Verify `sys.path.append()` |

### **Development Priorities**
1. **Functionality first**: Make it work
2. **Efficiency second**: Make it fast  
3. **Documentation third**: Make it clear

## ğŸ¯ **Efficient Interaction Patterns**

### **For Bug Fixes:**
```
1. ğŸ” Analyze error message â†’ Identify root cause
2. ğŸ¯ Show minimal fix â†’ Only changed lines
3. ğŸ’¡ Explain why â†’ Context for understanding
4. ğŸ’¾ STORE solution â†’ Save pattern in Memory for future reference
5. ğŸ–¥ï¸ Check terminals â†’ Reuse existing terminals, read outputs properly
```

### **For Feature Requests:**
```
1. ğŸ§  Use Sequential Thinking â†’ Break down complex requests
2. ğŸ“š Query DeepWiki â†’ Research software resources and computational approaches
3. ğŸ’¾ Store decisions â†’ Save insights in Memory
4. ğŸ”§ Implement â†’ Apply with context
5. ğŸ’¾ Store results â†’ Document outcomes and performance
6. ğŸ–¥ï¸ Terminal efficiency â†’ Reuse terminals, check output with get_terminal_output
```

### **For Architecture Questions:**
```
1. ï¿½ Search Memory first â†’ Check existing knowledge with search_nodes
2. ï¿½ğŸ“Š Create Mermaid diagram â†’ Visual representation
3. ğŸ¯ Highlight key components â†’ Focus on relevant parts
4. ğŸ”— Show relationships â†’ How pieces connect
5. ğŸ’¾ Store insights â†’ Save architectural decisions and trade-offs
```

## ğŸš€ Development Workflow

### **MCP-Enhanced Process**
1. **Problem Analysis** â†’ Sequential Thinking for complex issues
2. **Research** â†’ DeepWiki for software resources, computational guidance, and SOTA techniques across multiple repositories
3. **Implement** â†’ Apply insights
4. **ğŸ’¾ MANDATORY Memory Storage** â†’ **ALWAYS** store results, decisions, and insights
5. **Document** â†’ Markitdown for consistency

### **Key Patterns**
- **ğŸ’¾ CRITICAL: Always store results** in Memory server after ANY significant work
- **Query DeepWiki** for MoE implementations across research repositories (PyTorch, TensorFlow, JAX, research papers)
- **Use Sequential Thinking** for sparse gating, load balancing, optimization
- **Leverage Markitdown** for documentation enhancement

### **ğŸš¨ Memory Usage Rules - MANDATORY**
1. **After every experiment** â†’ Store results with `create_entities` or `add_observations`
2. **After solving bugs** â†’ Document solution patterns for future reference
3. **After architecture decisions** â†’ Record rationale and trade-offs
4. **Before major changes** â†’ Search existing knowledge with `search_nodes`
5. **Every session end** â†’ Update project progress and findings

## ğŸ’¾ **Memory Server Deep Dive**

### **ğŸ¯ When to Use Memory (ALWAYS!):**
- **Bug Solutions** â†’ Store error patterns and fixes
- **Experiment Results** â†’ Track all performance data
- **Architecture Decisions** â†’ Document design choices
- **Research Insights** â†’ Save SOTA findings
- **Code Patterns** â†’ Record effective implementations
- **Performance Benchmarks** â†’ Maintain historical data

### **ğŸ“Š Memory Entity Types:**
```python
# Core entity types for MOE Gym
"experiment_result"     # Performance data, configs, metrics
"bug_solution"          # Error patterns and fixes  
"architecture_decision" # Design choices and rationale
"research_insight"      # SOTA techniques and papers
"code_pattern"          # Reusable implementations
"performance_benchmark" # Historical comparisons
```

### **ğŸ”„ Memory Workflow Pattern:**
```python
# BEFORE any major work - Search existing knowledge
existing_knowledge = mcp_memory_search_nodes({"query": "relevant_topic"})

# DURING work - Consider storing intermediate insights
if significant_finding:
    mcp_memory_add_observations([{
        "entityName": "current_work_entity",
        "contents": [finding_description]
    }])

# AFTER work completion - MANDATORY storage
mcp_memory_create_entities([{
    "entityType": "appropriate_type",
    "name": "descriptive_name_with_date", 
    "observations": [result_summary, key_insights, recommendations]
}])
```

## ğŸ“‚ Project Structure
```
MOE-Gym/
â”œâ”€â”€ CIFAR10/model.py     # Dense MoE architecture
â”œâ”€â”€ CIFAR10/data.py      # Data loading & preprocessing  
â”œâ”€â”€ CIFAR10/train.py     # Training loops
â”œâ”€â”€ CIFAR10/run.py       # CLI interface
â”œâ”€â”€ CIFAR10-Sparse/     # NEW: Sparse MoE implementation
â”‚   â”œâ”€â”€ model.py        # Sparse MoE with top-k routing
â”‚   â”œâ”€â”€ data.py         # Data utilities
â”‚   â”œâ”€â”€ train.py        # Training with load balancing
â”‚   â”œâ”€â”€ run.py          # CLI for sparse experiments
â”‚   â””â”€â”€ Sparse_MoE_CIFAR10_Experiments.ipynb
â”œâ”€â”€ MOE_CIFAR10_Experiments.ipynb  # Dense MoE experiments
â””â”€â”€ .github/copilot-instructions.md # This file
```

## ğŸ“Š **Updated Performance Benchmarks (September 2025)**

### **Latest Results**
```mermaid
flowchart LR
    subgraph Dense["ğŸ”´ Dense MoE"]
        D1[4 Experts<br/>100% Active] --> D2[43% Accuracy<br/>100% FLOPs]
    end
    
    subgraph Sparse["ğŸ”µ Sparse MoE"] 
        S1[4 Experts<br/>k=2 Active] --> S2[35% Accuracy<br/>50% FLOPs]
        S3[8 Experts<br/>k=3 Active] --> S4[37% Accuracy<br/>37.5% FLOPs]
    end
    
    subgraph Optimal["â­ Sweet Spot"]
        O1[4 Experts, k=2<br/>Best efficiency] --> O2[7.9% accuracy loss<br/>50% compute savings]
    end
    
    classDef dense fill:#ffcdd2
    classDef sparse fill:#c8e6c9
    classDef optimal fill:#fff3e0
    
    class D2 dense
    class S2,S4 sparse
    class O2 optimal
```

### **Key Insights**
- **Optimal Configuration**: 4 experts with k=2 selection
- **Efficiency Trade-off**: ~8% accuracy loss for 50% computational savings
- **Scaling Behavior**: Diminishing returns beyond 4 experts
- **Load Balancing**: Critical for preventing expert collapse

## ğŸš€ **Updated Quick Commands (Tested September 2025)**

### **Working Commands**
```bash
# Sparse MoE - Core experiments
cd CIFAR10-Sparse
python run.py -n 4 -k 2 -d CIFAR10 -e 15 -b 32 -lr 0.001

# Pretrained backbone experiments  
python run.py -n 4 -k 2 -d CIFAR10 -e 15 -p --freeze-backbone -a 0.01

# Load balancing experiments
python run.py -n 8 -k 3 -d CIFAR10 -e 15 -a 0.05 --test-first

# With logging
python run.py -n 4 -k 2 -d CIFAR10 -e 15 -w --use-wandb
```

### **Current File Status**
```
CIFAR10-Sparse/           # Sparse MoE implementation  
â”œâ”€â”€ run.py               # âœ… Fixed variable naming conflicts (Sep 13, 2025)
â”œâ”€â”€ model.py             # Sparse routing with top-k selection
â”œâ”€â”€ train.py             # Load balancing loss integration
â”œâ”€â”€ data.py              # Pretrained data loading utilities
â””â”€â”€ Sparse_MoE_CIFAR10_Experiments.ipynb
```

## ğŸ”‘ Key Context
- **Device**: Auto-detect GPU/CPU
- **Input**: Flattened vectors (CIFAR-10: 3072, MNIST: 784)
- **Output**: Class logits (10 classes)
- **Training**: CrossEntropyLoss with Adam optimizer
- **Sparse Features**: Top-k routing, load balancing loss, expert utilization tracking
- **Logging**: Optional Wandb integration

## ğŸ”‘ Key Context
- **Device**: Auto-detect GPU/CPU
- **Input**: Flattened vectors (CIFAR-10: 3072, MNIST: 784)
- **Output**: Class logits (10 classes)
- **Training**: CrossEntropyLoss with Adam optimizer
- **Sparse Features**: Top-k routing, load balancing loss, expert utilization tracking
- **Logging**: Optional Wandb integration

---
**ğŸ‹ï¸ Built for researchers, by researchers. Always leverage MCP servers for optimal development!**

## Core Architecture (Updated)

### Dense vs Sparse MoE Comparison

```mermaid
flowchart LR
    subgraph Dense["ğŸ”´ Dense MoE (Original)"]
        D1[Input] --> D2[Gate Network]
        D1 --> D3[Expert 1]
        D1 --> D4[Expert 2]
        D1 --> D5[Expert 3]
        D1 --> D6[Expert 4]
        
        D2 --> D7[All Expert Weights]
        D3 --> D8[Weighted Sum<br/>ALL experts used]
        D4 --> D8
        D5 --> D8
        D6 --> D8
        D7 --> D8
        D8 --> D9[Output<br/>FLOPs: 100%]
    end
    
    subgraph Sparse["ğŸ”µ Sparse MoE (New)"]
        S1[Input] --> S2[Sparse Gate]
        S1 --> S3[Expert 1]
        S1 --> S4[Expert 2]
        S1 --> S5[Expert 3]
        S1 --> S6[Expert 4]
        
        S2 --> S7[Top-k Selection<br/>k=2 experts]
        S7 --> S8[Selected Weights]
        S3 --> S9[Sparse Combination<br/>ONLY k experts]
        S4 --> S9
        S8 --> S9
        S9 --> S10[Output<br/>FLOPs: 50%]
        
        S5 -.-> S11[Inactive]
        S6 -.-> S11
    end
    
    classDef dense fill:#ffcdd2
    classDef sparse fill:#c8e6c9
    classDef inactive fill:#f5f5f5,stroke-dasharray: 5 5
    
    class D9 dense
    class S10 sparse
    class S11 inactive
```

### Key Components
- **Experts**: Individual neural networks (`get_experts()` in `model.py`) - each expert is a 4-layer fully connected network (64â†’128â†’256â†’10) using `nn.Sequential`
- **Gate**: Gating network (`get_gate()` in `model.py`) that determines expert weights for each input using `nn.Sequential`
- **MOE_Model**: Custom PyTorch `nn.Module` that combines expert outputs using gate weights via matrix multiplication

### Critical Design Patterns

**Flattened Input Convention**: All inputs are flattened to 1D vectors:
- MNIST: `784` features (28*28)
- CIFAR-10: `3072` features (32*32*3)

**Sparse Routing Logic** (`model.py` forward method):
```python
# True sparse routing - selection BEFORE expert computation
top_k_indices, top_k_weights, router_logits = self.gate(inputs)

# Route ONLY to selected experts
for expert_idx in range(len(self.experts)):
    expert_mask = (top_k_indices == expert_idx)
    if expert_mask.any():  # Only compute if expert is selected
        expert_inputs = inputs[sample_indices]
        expert_output = self.experts[expert_idx](expert_inputs)
        # Computational savings: only k/n experts compute
```
    
    I --> J[Final Prediction<br/>Shape: batch_size x 10]
    
    style A fill:#e1f5fe
    style J fill:#c8e6c9
    style I fill:#fff3e0
```

### Key Components
- **Experts**: Individual neural networks (`get_experts()` in `model.py`) - each expert is a 4-layer fully connected network (64â†’128â†’256â†’10) using `nn.Sequential`
- **Gate**: Gating network (`get_gate()` in `model.py`) that determines expert weights for each input using `nn.Sequential`
- **MOE_Model**: Custom PyTorch `nn.Module` that combines expert outputs using gate weights via matrix multiplication

### Critical Design Patterns

**Flattened Input Convention**: All inputs are flattened to 1D vectors:
- MNIST: `784` features (28*28)
- CIFAR-10: `3072` features (32*32*3)

**Weighted Combination Logic** (`model.py` forward method):
```python
gates = self.gate(inputs).unsqueeze(-1)  # [batch_size, n_experts, 1]
expert_outputs = []
for expert in self.experts:
    expert_outputs.append(expert(inputs))
values = torch.stack(expert_outputs, dim=-1)  # [batch_size, 10, n_experts]
return torch.matmul(values, gates).squeeze(-1)  # [batch_size, 10]
```

## ğŸ¯ **MCP-Enhanced Development Examples**

### **Example 1: Implementing Sparse Gating (COMPLETE WORKFLOW)**

**Problem**: User asks "How to implement sparse gating?"

**ğŸ”¥ COMPREHENSIVE MCP-Enhanced Approach:**
```python
# STEP 1: Strategic Planning
response = mcp_sequentialthi_sequentialthinking({
    "thought": "Analyze sparse gating requirements: top-k selection, load balancing, gradient flow, computational efficiency",
    "nextThoughtNeeded": true,
    "thoughtNumber": 1,
    "totalThoughts": 8
})

# STEP 2: Multi-Source Research
sota_research = mcp_deepwiki_ask_question({
    "repoName": "huggingface/transformers",
    "question": "How is sparse MoE implemented in Switch Transformer, Mixtral, and GLaM?"
})

pytorch_research = mcp_deepwiki_ask_question({
    "repoName": "pytorch/pytorch", 
    "question": "PyTorch implementations of sparse routing mechanisms and top-k operations"
})

fairseq_research = mcp_deepwiki_ask_question({
    "repoName": "facebookresearch/fairseq",
    "question": "MoE layer implementations, gating strategies, and load balancing techniques"
})

# STEP 3: Web Research for Latest Trends
web_research = fetch_webpage({
    "query": "sparse mixture of experts 2025 SOTA techniques efficiency",
    "urls": ["https://arxiv.org/search/?query=sparse+mixture+of+experts&searchtype=all"]
})

# STEP 4: Technical Documentation
docs_research = mcp_context7_get_library_docs({
    "context7CompatibleLibraryID": "/pytorch/pytorch",
    "topic": "distributed training sparse operations"
})

# STEP 5: Code Analysis for Current Implementation
syntax_check = mcp_pylance_mcp_s_pylanceFileSyntaxErrors({
    "fileUri": "file:///path/to/current/model.py",
    "workspaceRoot": "file:///path/to/workspace"
})

# STEP 6: Store Research Findings
mcp_memory_create_entities([{
    "entityType": "research_synthesis",
    "name": "sparse_gating_comprehensive_research_2025",
    "observations": [
        f"SOTA techniques: {sota_research.key_findings}",
        f"PyTorch implementations: {pytorch_research.best_practices}",
        f"Fairseq patterns: {fairseq_research.efficient_methods}",
        f"Latest trends: {web_research.emerging_techniques}",
        f"Technical considerations: {docs_research.optimization_tips}"
    ]
}])

# STEP 7: Implementation with Context
def implement_sparse_gating():
    # Apply insights from comprehensive research
    # Include load balancing, efficiency optimizations
    # Follow SOTA patterns from literature review
    pass

# STEP 8: Documentation and Storage
implementation_docs = mcp_markitdown_convert_to_markdown({
    "uri": "implementation_summary.md"
})

mcp_memory_add_observations([{
    "entityName": "sparse_gating_comprehensive_research_2025",
    "contents": [
        "Implementation completed with SOTA techniques",
        "Performance benchmarks and optimization notes",
        "Future improvement recommendations"
    ]
}])
```

### **Example 2: Performance Analysis & Optimization**

**Problem**: User wants comprehensive performance analysis

**ğŸ”¥ SYSTEMATIC MCP APPROACH:**
```python
# STEP 1: Historical Context
historical_data = mcp_memory_search_nodes({
    "query": "experiment results performance benchmarks accuracy efficiency"
})

# STEP 2: Current State Analysis
current_metrics = mcp_pylance_mcp_s_pylanceWorkspaceUserFiles({
    "workspaceRoot": "file:///workspace"
})

# STEP 3: Industry Benchmarks Research
benchmark_research = mcp_deepwiki_ask_question({
    "repoName": "microsoft/DeepSpeed",
    "question": "MoE performance benchmarks, FLOPs analysis, and efficiency metrics"
})

# STEP 4: Infrastructure Optimization
infra_analysis = mcp_hashicorp_ter_search_modules({
    "module_query": "gpu optimization distributed training"
})

# STEP 5: Store Comprehensive Analysis
mcp_memory_create_entities([{
    "entityType": "performance_analysis",
    "name": f"comprehensive_performance_review_{datetime.now().strftime('%Y%m%d')}",
    "observations": [
        f"Historical trends: {historical_data.patterns}",
        f"Current bottlenecks: {current_metrics.issues}",
        f"Industry benchmarks: {benchmark_research.standards}",
        f"Optimization opportunities: {infra_analysis.improvements}",
        f"Recommendations: {generate_recommendations()}"
    ]
}])
```

### **Example 3: Security & Quality Assurance**

**Problem**: Comprehensive security and quality review

**ğŸ”¥ MULTI-TOOL SECURITY WORKFLOW:**
```python
# STEP 1: Code Security Analysis
security_scan = mcp_pylance_mcp_s_pylanceInvokeRefactoring({
    "fileUri": "file:///workspace/model.py",
    "name": "source.fixAll.pylance"
})

# STEP 2: Vulnerability Research
vuln_research = mcp_deepwiki_ask_question({
    "repoName": "OWASP/owasp-mastg",
    "question": "ML model security best practices, input validation, and threat mitigation"
})

# STEP 3: Infrastructure Security
infra_security = mcp_hashicorp_ter_search_policies({
    "policy_query": "security compliance machine learning"
})

# STEP 4: Dependency Analysis
deps_check = mcp_pylance_mcp_s_pylanceImports({
    "workspaceRoot": "file:///workspace"
})

# STEP 5: Documentation Security Review
security_docs = mcp_markitdown_convert_to_markdown({
    "uri": "security_checklist.md"
})

# STEP 6: Comprehensive Security Report
mcp_memory_create_entities([{
    "entityType": "security_audit",
    "name": f"comprehensive_security_review_{datetime.now().strftime('%Y%m%d')}",
    "observations": [
        f"Code analysis results: {security_scan.findings}",
        f"Security best practices: {vuln_research.recommendations}",
        f"Infrastructure compliance: {infra_security.policies}",
        f"Dependency vulnerabilities: {deps_check.risks}",
        f"Remediation plan: {generate_security_plan()}"
    ]
}])
```

---

## ğŸš€ **INTEGRATION SUCCESS METRICS**

**Track MCP tool utilization effectiveness:**

```python
mcp_effectiveness_metrics = {
    "research_quality": "Sources consulted per decision",
    "code_quality": "Pylance issues resolved per commit", 
    "knowledge_retention": "Memory entities created per session",
    "documentation_completeness": "Markitdown conversions per feature",
    "infrastructure_reliability": "Terraform modules deployed successfully",
    "security_posture": "Vulnerabilities identified and resolved",
    "collaboration_efficiency": "Communication tools automated",
    "continuous_improvement": "Feedback loops implemented"
}
```

**ğŸ¯ SUCCESS INDICATORS:**
- âœ… **Zero manual research** - All insights from MCP tools
- âœ… **Zero undocumented decisions** - All stored in Memory
- âœ… **Zero security oversights** - Comprehensive scanning
- âœ… **Zero deployment failures** - Infrastructure as code
- âœ… **Zero knowledge loss** - Persistent storage and retrieval
- âœ… **Zero repetitive work** - Automation and templates
- âœ… **Zero isolation** - Always use full toolchain

**ğŸ”¥ ULTIMATE GOAL: Every task becomes a comprehensive, well-researched, secure, documented, and scalable solution using the COMPLETE MCP ecosystem!**
    "repoName": "facebookresearch/fairseq",
    "question": "MoE layer implementations and gating strategies"
})

# Step 3: Store design decisions
mcp_memory_create_entities([{
    "entityType": "design_decision",
    "name": "sparse_gating_implementation",
    "observations": ["Top-k expert selection", "Load balancing loss", "Gradient handling"]
}])

# Step 4: Implement with context
```

### **Example 2: Performance Analysis**

**Problem**: User wants to compare different expert configurations

**MCP-Enhanced Approach:**
```python
# STEP 1: Search existing experiments first
historical_data = mcp_memory_search_nodes({"query": "experiment results accuracy performance"})

# STEP 2: Store experiment results automatically
def store_experiment_result(config, results):
    mcp_memory_add_observations([{
        "entityName": "experiment_results",
        "contents": [f"{config}: {results['accuracy']:.2f}% accuracy, {results['loss']:.4f} loss, {results['flops']:.1f}% FLOPs"]
    }])

# STEP 3: Query historical results
def get_best_configuration():
    historical_data = mcp_memory_search_nodes({"query": "experiment results accuracy"})
    return analyze_patterns(historical_data)

# STEP 4: Store comparison analysis
mcp_memory_create_entities([{
    "entityType": "performance_analysis", 
    "name": f"config_comparison_{datetime.now().strftime('%Y%m%d')}",
    "observations": [
        f"Best config: {best_config} with {best_accuracy}% accuracy",
        f"Trade-off analysis: {efficiency_vs_accuracy_summary}",
        f"Recommendations: {optimization_suggestions}"
    ]
}])
```

### **Example 3: Architecture Research**

**Problem**: User wants to understand current MoE trends

**MCP-Enhanced Approach:**
```python
# Research latest developments from multiple sources
research_data = mcp_deepwiki_ask_question({
    "repoName": "huggingface/transformers",
    "question": "What are the latest MoE architectures and their performance characteristics?"
})

# Research PyTorch-specific implementations
pytorch_research = mcp_deepwiki_ask_question({
    "repoName": "pytorch/pytorch",
    "question": "Latest PyTorch MoE implementations and optimization techniques"
})

# Research from specialized MoE repositories
moe_research = mcp_deepwiki_ask_question({
    "repoName": "microsoft/DeepSpeed",
    "question": "DeepSpeed MoE training optimizations and scaling techniques"
})

# Convert findings to structured documentation
structured_docs = mcp_markitdown_convert_to_markdown({
    "uri": research_data.source_url
})

# Store insights for future reference
mcp_memory_create_entities([{
    "entityType": "research_insight",
    "name": "current_moe_trends_2025",
    "observations": [research_data.summary, pytorch_research.summary, moe_research.summary]
}])
```

## Development Workflows

### **ğŸ”„ MCP-Integrated Workflow**

1. **Problem Analysis** â†’ Use Sequential Thinking for complex problems
2. **Research Phase** â†’ Query DeepWiki for SOTA techniques  
3. **Implementation** â†’ Apply insights with Memory context
4. **Documentation** â†’ Use Markitdown for consistency
5. **Results Storage** â†’ Store in Memory for future reference

### Dependencies
Install required packages:
```bash
pip install torch torchvision wandb  # Core PyTorch dependencies
```

### Running Experiments  
Use `run.py` as the main entry point:
```bash
python run.py -n 4 -d CIFAR10 -e 30 -w  # 4 experts, CIFAR-10, 30 epochs, with wandb
python run.py -n 1 -d MNIST -e 10       # Single expert baseline on MNIST
```

### Training Philosophy
- **Classification**: Uses standard `nn.CrossEntropyLoss` with custom PyTorch training loop
- **Device Management**: Automatic GPU detection and tensor placement
- **Regression Adaptation**: Custom training loop code exists but is commented out - the loss function differs for regression (mixture of Gaussians vs. mixture of softmax)

## Project-Specific Conventions

### Model Instantiation Pattern
Create model and setup optimizer (see `run.py` lines 32-38):
```python
m = model.MOE_Model(
    model.get_experts(n_experts, input_dim), 
    model.get_gate(n_experts, input_dim)
)
m.optimizer = torch.optim.Adam(m.parameters(), lr=0.001)
```

### Data Pipeline
- Both datasets use `torchvision.datasets` with custom transforms for flattening
- One-hot encoding using `F.one_hot()` for compatibility with original architecture
- `DataLoader` with automatic batching and shuffling
- Device placement handled in training loop

### Device Management
Automatic GPU detection:
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

### Wandb Integration
Optional experiment tracking via `--use-wandb` flag. Config automatically includes experts count, dataset, and epochs.

## Key Files
- `model.py`: Core MoE architecture using PyTorch `nn.Module` and custom training utilities
- `data.py`: Dataset loading using `torchvision` with DataLoader and tensor preprocessing
- `train.py`: PyTorch training loop with loss computation and wandb logging  
- `run.py`: CLI interface with device detection and model setup
- `images/`: Contains performance graphs referenced in README

## Documentation Changes
> **Note**: This section tracks changes to project documentation structure

### Recent Updates
- **[2025-09-12] Complete PyTorch Migration**: Converted entire codebase from TensorFlow to PyTorch
  - Replaced Keras layers with `nn.Module` architecture
  - Updated data loading from `tf.data` to `torchvision.datasets` + `DataLoader`
  - Migrated from `model.fit()` to custom PyTorch training loops
  - Added automatic GPU detection and device management
- Added `.github/copilot-instructions.md` with comprehensive AI agent guidance
- Included Mermaid diagrams for visual architecture flow
- Documented critical design patterns and development workflows

### Documentation Structure
```mermaid
flowchart LR
    A[README.md<br/>Main project overview] --> B[.github/copilot-instructions.md<br/>AI agent guidance]
    A --> C[images/<br/>Performance graphs]
    B --> D[Architecture diagrams<br/>Development workflows]
    C --> E[Validation results<br/>Training dynamics]
    
    classDef readme fill:#e3f2fd
    classDef instructions fill:#f3e5f5
    classDef images fill:#e8f5e8
    
    class A readme
    class B instructions
    class C images
```

---
**ğŸ‹ï¸ Built for researchers, by researchers. Always leverage MCP servers for optimal development!**

**Last Updated**: September 13, 2025 - Added token efficiency guidelines and bug fix patterns